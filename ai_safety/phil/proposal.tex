\include{preamble}
\title{Anthropomorphic Algorithms for AI Safety}
\author[Tom Wallis]{\href{http://tom.coffee}{Tom Wallis}}
\date{}

\begin{document}

\maketitle

\section{AI Safety and Existential Risk}

% Corrigibility is a big problem for existential risk and AI safety in general
% It's not the only problem:
% - Concrete problems paper
% - Ethical and roboethical concerns
\newthought{A general artificial intelligence}, should it be constructed, would be very dangerous. This is rather well documented. For example, a recent paper discussing an artificial intelligence's corrigibility\cite{corrigability} demonstrates an issue where a general artificial intelligence may resist human control. If this agent was dangerous --- which is probable --- then the problem of corrigibility becomes very important indeed.\par

\newthought{Other problems} exist, too. We are aware of some concrete problems in AI which experts can work on solving today\cite{concrete_problems} --- one particularly interesting example is that of Reward Hacking, where an agent might ``cheat'' its reward functions in order to achieve its goals. While this could be harmless or inconvenient at best (a cleaning robot, say, which shuts its eyes and believes no mess exists because it can't see any), it could be devastating at worst. Unprecedented action which \emph{technically} achieves goals but inadvertently causes other problems (or immediate harm) could be cataclysmic when enacted by a sufficiently intelligent agent.\par

% Anthropomorphic algorithms pose a potential solution
\newthought{In my research} this year on computational responsibility formalisms --- algorithms which imbue an intelligent agent with a sense of ``responsibility'' as it chooses actions to achieve its goals --- I believe I have found an interesting opportunity to solve these problems using what I term ``anthropomorphic algorithms''. Anthropomorphic Algorithms are algorithms which simulate a social human trait in an artificial agent. Examples of anthropomorphic algorithms already widely researched would be ones termed ``computational trust'', versions of which are now several decades old\cite{marsh1994}, but rarely researched outside of sociological and sociotechnical research. I believe that an application of these algorithms lies in researching and implementing philosophical theories of AI safety. \par

\section{Anthropomorphic Algorithms and Philosophy: Solving Problems}

% Algorithmics -> Michael Devitt's experimental semantics
% - There's a theory to put together on anthropomorphic algorithms.
% - We can begin to theorise about how to control & design intelligent
%       agents with an eye to testing these ideas. 
\newthought{While there are} computer science researchers attempting to solve the noted AI safety problems algorithmically, philosophical work frequently produces fewer practical results due to its often metaphysical nature.\par

However, this doesn't have to be the case. Michael Devitt's work in experimental semantics\cite{experimental_semantics} is a shining example of philosophical research which is proven through data and concrete, repeatable examples. Computational responsibility, and anthropomorphic algorithms in general, afford other avenues to test philosophical theory by.\par

Indeed, anthropomorphic algorithms provide other problems for philosophy to solve: the claim of imbuing a computer with human traits is a contentious one, and as computer science, sociology and psychology continue to refine formalisms of ordinarily human traits, the necessity of philosophical literature on the topic increases proportionally.\par
% While there *are* concrete problems, they're too complex for CS or philosophy
%     on their own.
% - We need to combine the two
% This is an exciting opportunity to solve real & important problems,
%     further experimental philosophy, and combine literature on philosophy
%     and CS in helpful ways.
\newthought{The problem of} AI safety --- while one of existential risk --- is also one requiring collaboration between various fields, including philosophy, computer science, psychology, political science\sidenote{An interesting political question related to general artificial intelligence presents itself: should we have an intelligence roughly equal to that of humans, this introduces the social quandary of rights. Intelligence seems to be the important factor in the allocation of rights. Some countries award rights to animals like dolphins and intelligent apes, but less intelligent animals are less often afforded this thought.} and others; this collaboration can be difficult to organise, particularly when lacking a common frame of reference such as jargon or theory.\quiettodo{Is this sentence too long?} Fortunately, anthropomorphic algorithms provide a framework for interdisciplinary research between all of these fields.\par

I propose that a significant body of philosophical literature stands to be written on the subject. Particularly, I am excited to investigate the impact of anthropomorphic algorithms on the corrigibility of intelligent agents, as well as their application to the solution of the reward hacking problem, and have a working paper detailing a technique which allows just this. The method presented is unusual, in that it constrains behaviour through analysis with that of humans --- in applying anthropomorphic algorithms --- rather than tweaking aspects of the AI engineering itself. Introducing human-like traits to an artificial agent may make it controllable via indirect means. It could also be used to demonstrate artificial agents which are capable of reward hacking, but unwilling to act on this ability due to an ingrained sense of, for example, responsibility.\par

\section{Proposed Work}
\newthought{Specific work} I intend to work on during a DPhil at Oxford University involves developing further anthropomorphic formalisms, performing interdisciplinary research in order to assert the applicability of a formalism to the problem of AI safety, and further developing the technique as it pertains to both AI Safety and theories of mind. The formalisms developed would be used to further the relevant philosophical and computational technique of my own design, which presents new ways to analyse the space of possible minds and approach AI safety through behaviour constraint.\par

Problems to be solved in this work involve the application of the theory to a broader range of theories of mind, as well as asserting a-priori its application to AI Safety and its validity as an empirical philosophical tool.\par

Finally, the research might involve an analysis of the implications of the technique's abstraction of behaviour over the space of possible minds. This would provide a stronger argument backing up the theory, alongside a technique by corollary for creating abstractions over the space of possible minds, thereby avoiding problems with the space's indeterminate structure. As the space of possible minds is an important and common concept, gaining analytical value from the space without specifying its structure is certain to have utility far beyond what is developed over the course of the DPhil, and would constitute a significant advancement in the field on its own.\par

\newpage

\section{Personal Statement}

% Interest in existential risk
% Writing and interest in unusual interdisciplinary study
% - Project Albert
% Research experience and experience with anthropomorphics
\newthought{Given my experience} developing the first computational responsibility formalism, I am uniquely equipped to begin the proposed research. The formalism I have designed has been purposefully created with a philosophical foundation in mind, drawing from work by P.F. Strawson\cite{freedomandresentment} and Thomas Scanlon\cite{scanlon2006justice}, and an equal foundation in computational disciplines such as machine learning and sociotechnical systems modelling\cite{sommerville_resp_depend}. Therefore, its design lends itself to the testing of philosophical theories while relating to computer science research on artificial intelligence rather nicely. The formalism is inspired in its design by Marsh's seminal model of computational trust\cite{marsh1994}, which itself was inspired by mathematics, psychology and sociology --- an intention of its design was its inter-disciplinary potential as a research tool. The formalism of responsibility, then, follows in Marsh's ideological footsteps, but with a philosophy-centric focus.\par

\newthought{Aside from anthropomorphic algorithms}, I have other research experience relevant to the AI safety work proposed.\par
One example of this is the experimental literature project I run extra-curricularly, \emph{Project Albert}\sidenote{Information on Project Albert can be found at \href{http://projectalbert.net/}{http://projectalbert.net/}.}. Project Albert is an application of systems design techniques to children's bedtime story improvisation. I am particularly interested in experimental techniques for humanities research, much in the vein of Michael Devitt's experimental semantics\sidenote{An essay on the work so far and its efficacy is currently in early development, and can be found \href{https://github.com/probablytom/Albert-paper}{at http://bit.ly/2fZvggr}.} --- I look forward to applying this mindset to philosophy research also, particularly as the proposed research is fundamentally interdisciplinary.\par

Another relevant research project would be my sociotechnical systems modelling project for my honours year\sidenote{This work won the prize for ``\smallcaps{best software product}'' for my year's honours projects.}. This project applied a novel modelling approach for sociotechnical systems to a new system for injecting variance into those models --- both of which were my own design\sidenote{A paper on this work is currently being developed, and can be found \href{https://github.com/probablytom/fuzzi-moss-paper}{at http://bit.ly/2gi4GD0}.}. The project then used these new approaches to insert human-like mistakes in the artificial agents modelled. Sociotechnical modelling, as well as and the representation of human traits in these models, naturally lend themselves to the proposed research.\par

\newthought{Taking this experience} into account, and having an intricate understanding of the development and design of anthropomorphic algorithms, I am certain I am an ideal candidate to undertake the essential AI safety research pertaining to this novel technique. Not only have I designed myself the responsibility formalism which gives rise to a new method to tackle problems of reward hacking and corrigibility, but I have also undertaken award-winning research representing human traits in the past, and currently research experimental humanities techniques which provide an ideal background to begin experimental philosophy research. The shift in my current research to a philosophical focus indicates that the locus of my own attention should shift accordingly, and Oxford University --- with a rich AI Safety and interdisciplinary research community --- provides the most fertile intellectual ground for this work to flourish. Indeed, the achievement of complete drafts of a working paper in philosophy\cite{wallis_2016}, having undertaken only half of the research so far, demonstrates both my aptitude for philosophical study and the very real potential for prolific development of a new angle in AI Safety and theories of mind.\par

\newthought{It is a certain fact} that Oxford University presents the perfect environment to pursue this research, through the work done by the Future of Humanity Institute especially. Not only does Oxford University promote a healthy interdisciplinary research environment, but its experts in the field --- some of whom are cited in this very document --- lead the international research community in this area. Aside from the honour of learning under these global leaders of the field, the task of refining and developing these behavioural techniques is unusually complex and nuanced --- it \emph{requires} their guidance to lay proper groundwork, as its contribution to the field has much potential.\par

Oxford University may also be the only suitable candidate as a location to pursue this research. Few institutions display the open-minded approach to such a combination of philosophy with computing science, and Oxford University appears to far outstrip other places of research in its focus on AI Safety as an existential risk threat, combined with the success of the Future of Humanity Institute in highlighting and addressing problems in the field. If this research is to be conducted anywhere, it must be Oxford.\par

I am excited to begin the philosophical work, and doubtless that the opportunities it permits will be not only vast, but deeply fascinating, and of great impact.\par\bigskip

Thank you for your consideration.

\bigskip
\begin{flushright}
    Cordially,\\
    Tom Wallis.
\end{flushright}

\nobibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
