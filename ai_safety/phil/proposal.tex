\include{preamble}
\title{Anthropomorphic Algorithms for AI Safety}
\author[Tom Wallis]{\href{http://tom.coffee}{Tom Wallis}}
\date{}

\begin{document}

\maketitle

\section{AI Safety and Existential Risk}

% Corrigability is a big problem for existential risk and AI safety in general
% It's not the only problem:
% - Concrete problems paper
% - Ethical and roboethical concerns
\newthought{A general artificial intelligence}, should it be constructed, would be very dangerous. This is rather well documented. For example, a recent paper discussing an artificial intelligence's corrigibility\cite{corrigability} demonstrates an issue where a general artificial intelligence may resist human control. If this agent was dangerous --- which is probable --- then the problem of corrigibility becomes very important indeed.\par

\newthought{Other problems} exist, too. We are aware of some concrete problems in AI which experts can work on solving today\cite{concrete_problems} --- one particularly interesting example is that of Reward Hacking, where an agent might ``cheat'' its reward functions in order to achieve its goals. While this could be harmless or inconvenient at best (a cleaning robot, say, which shuts its eyes and believes no mess exists because it can't see any), it could be devastating at worst. Unprecidented action which \emph{technically} achieves goals but inadvertantly causes other problems (or immediate harm) could be cataclysmic when enacted by a sufficiently intelligent agent.\par

% Anthropomorphic algorithms pose a potential solution
\newthought{In my research} this year on computational responsibility formalisms --- algorithms which imbue an intelligent agent with a sense of ``responsibility'' as it chooses actions to achieve its goals --- I believe I have found an interesting opportunity to solve these problems using what I term ``anthropomorphic algorithms''. Anthropomorphic Algorithms are algorithms which simulate a social human trait in an artificial agent. Examples of anthropomorphic algorithms already widely researched would be ones termed ``computational trust'', versions of which are now several years old\cite{marsh1994}, but rarely researched outside of sociological and sociotechnical research. I believe that an application of these algorithms lies in researching and implementing philosophical theories of AI safety. \par

\section{Anthropomorphic Algorithms and Philosophy: Solving Problems}

% Algorithmics -> Michael Devitt's experimental semantics
% - There's a theory to put together on anthropomorphic algorithms.
% - We can begin to theorise about how to control & design intelligent
%       agents with an eye to testing these ideas. 
\newthought{While there are} computer science researchers attempting to solve the noted AI safety problems algorithmically, philosophical work frequently produces fewer practical results due to its often metaphysical nature.\par

However, this doesn't have to be the case. Michael Devitt's work in experimental semantics\cite{experimental_semantics} is a shining example of philosophical research which is backed by data and concrete, repeatable examples. Computational responsibility, and anthropomorphic algorithms in general, afford other avenues to test philosophical theory by.\par

Indeed, anthropomorphic algorithms provide other problems for philosophy to solve: the claim of imbuing a computer with human traits is a contentious one, and as computer science, sociology and psychology continue to refine formalisms of ordinarily human traits, the necessity of philosophical literature on the topic increases proportionally.\par
% While there *are* concrete problems, they're too complex for CS or philosophy
%     on their own.
% - We need to combine the two
% This is an exciting opportunity to solve real & important problems,
%     further experimental philosophy, and combine literature on philosophy
%     and CS in helpful ways.
\newthought{The problem of} AI safety --- while one of existential risk --- is also one requiring collaboration between various fields, including philosophy, computer science, psychology, political science\sidenote{An interesting political question related to general artificial intelligence presents itself: should we have an intelligence roughly equal to that of humans, this introduces the social quandry of rights. Intelligence seems to be the important factor in the allocation of rights. Some countries award rights to animals like dolphins and intelligent apes, but less intelligent animals are less often afforded this thought.} and others; this collaboration can be difficult to organise, particularly when lacking a common frame of reference such as jargon or theory.\todo{Is this sentence too long?} Fortunately, anthropomorphic algorithms provide a framework for interdisciplinary research between all of these fields.\par

I propose that a significant body of philosophical literature stands to be written on the subject. Particularly, I am excited to investigate the impact of anthropomorphic algorithms on the corrigibility of intelligent agents, as well as their application to the solution of the reward hacking problem. Introducing human-like traits to an artificial agent may make it controllable via indirect means. It could also be used to demonstrate artificial agents which are capable of reward hacking, but unwilling to act on this ability due to an ingrained sense of responsibility. I am also interested in researching the problems in roboethics and moral responsibility arising from the introduction of anthropomorphic agents.\par

\section{My suitability}

% Interest in existential risk
% Writing and interest in unusual interdisciplinary study
% - Project Albert
% Research experience and experience with anthropomorphics
\newthought{Given my experience} developing the first computational responsibility formalism, I am uniquely equipped to begin the proposed research. The formalism I have designed has been purposefully created with a philosophical foundation in mind, drawing from work by P.F. Strawson\cite{freedomandresentment} and Thomas Scanlon\cite{scanlon2006justice}, and an equal foundation in computational disciplines such as machine learning and sociotechnical systems modelling\cite{sommerville_resp_depend}. Therefore, its design leds itself to the testing of philosophical theories while relating to computer science research on artificial intelligence rather nicely. The formalism is inspired in its design by Marsh's seminal model of computational trust\cite{marsh1994}, which itself was inspired by mathematics, psychology and sociology --- an intention of its design was its inter-disciplinary potential as a research tool. The formalism of responsibility, then, follows in Marsh's ideological footsteps, but with a philosophy-centric focus.\par

\newthought{Aside from anthropomorphic algorithms}, I have other research experience relevant to the AI safety work proposed.\par
One example of this is the experimental literature project I run extra-curricularly, \emph{Project Albert}\sidenote{Information on Project Albert can be found at \href{http://projectalbert.net/}{http://projectalbert.net/}.}. Project Albert is an application of systems design techniques to children's bedtime story improvisation. I am particularly interested in experimental techniques for humanities research, much in the vein of Michael Devitt's experimental semantics\sidenote{An essay on the work so far and its efficacy is currently in early development, and can be found \href{https://github.com/probablytom/Albert-paper}{at http://bit.ly/2fZvggr}.\todo{Does the ``and'' here read okay?}} --- I look forward to applying this mindset to philosophy research also.\par
Another relevant research project would be my sociotechnical systems modelling project for my honours year\sidenote{This work won the prize for ``\smallcaps{best software product}'' for my year's honours projects.\todo{\smallcaps{best software product} or \emph{best software product}?}}. This project applied a novel modelling approach for sociotehcnical systems to a new system for injecting variance into those models --- both of which were my own design\sidenote{A paper on this work is currently being developed, and can be found \href{https://github.com/probablytom/fuzzi-moss-paper}{at http://bit.ly/2gi4GD0}\todo{And or which?}.}. The project then used these new approaches to insert human-like mistakes in the artificial agents modelled. Sociotechnical modelling, as well as and the representation of human traits in these models, naturally lend themselves to the proposed research.\par

\newthought{Taking this experience} into account, and having an intricate understanding of the development and design of anthropomorphic algorithms, I am certain I am an ideal candidate to undertake the essential AI safety research pertaining to this novel technique. Not only have I designed myself the responsibility formalism which gives rise to a new method to tackle problems of reward hacking and corrigibility, but I have also undertaken award-winning research representing human traits in the past, and currently research experimental humanities techniques which provide an ideal background to begin experimental philosophy research. I am excited to begin the philosophical work, and doubtless that this new work it permits will be not only vast, but deeply fascinating.\par
Thanks you for your consideration.

\bigskip
\begin{flushright}
    Cordially,\\
    Tom Wallis.
\end{flushright}

% \newthought{The first}, \emph{Project Albert}\sidenote{Information on Project Albert can be found at \href{http://projectalbert.net/}{http://projectalbert.net/}.}, a research project in experimental storytelling. Not unlike Michael Devitt's work on experimental semantics, Project Albert seeks to employ systems design techniques to the improvisation of childrens' bedtime stories. Aside from the societal and practical benefits, this was an experiment with the intention of taking a humanities subject --- here literature --- and applying a practical experimental technique to it.\sidenote{An essay on the work so far and its efficacy is currently in early development, which can be found \href{https://github.com/probablytom/Albert-paper}{at http://bit.ly/2fZvggr}.} I hope to apply a similar mindset to philosophical work on artificial intelligence. I believe this holds great value for the field: it is imperative that philosophical work on AI safety is carried out, and that this research be made testable and practical will ease interdisciplinary research with another vital field for the area, computing science (in which I am also well versed).\par
% 
% \newthought{The second} research experience to note is work done in sociotechnical systems modelling: the analysis and simulation of complex systems of people and the technology they interact with. Naturally, artificial agents (particularly anthropomorphic ones) interacting in the day-to-day world of humans is a prime example of a sociotechnical system on our cultural horizon. \sidenote{A paper on this work is currently being developed, which can be found \href{https://github.com/probablytom/fuzzi-moss-paper}{at http://bit.ly/2gi4GD0}.}My research was on developing a new modelling framework for sociotechnical systems, which injected human-like variance to a model of a system so as to properly simulate what happens when introducing unpredictable human error in a model. \sidenote{This work won the prize for ``best software product'' for my year's honours projects.}While my masters in computing science lends itself to understanding the nuances of AI development itself, my experience modelling people with technology --- and modelling complex systems with human traits --- lends itself to a great deal of understanding of the impact of intelligent agents in the wider society, meaning I have a broad range of perspectives to employ in philosophical research on the topic.\par


% The first, sociotechnical systems, involves analysis of complex systems of people and technology they interact with; my work was to create a modelling system which allowed simulation of sociotechnical workflows without barriers to entry like understanding of a domain-specific modelling language.\sidenote{A paper on this work is currently being developed, which can be found \href{https://github.com/probablytom/fuzzi-moss-paper}{at http://bit.ly/2gi4GD0}. My original dissertation can be found \href{https://github.com/probablytom/diss-doc/blob/master/Honours_Dissertation.pdf}{at http://bit.ly/2fYbcgy}.} These models were then dynamically altered during runtime by a code fuzzer I designed and implemented, which injected human-like variance into the model, allowing for accurate simulations of human behaviour using simple modelling techniques. This research won an award for ``Best Software Product'' for my year. I believe that my experience researching complex systems of this nature natually lends itself to existential risk research.\par

% \newthought{Other research} I pursue lies in the field of experimental storytelling. \sidenote{Information on Project Albert can be found at \href{http://projectalbert.net/}{http://projectalbert.net/}.}Project Albert explores the possibility that improvisation of children's bedtime stories might be made easier and more accessible by use of design patterns, which generalise complex ideas by turning them into interrelated rules which are defined semantically.\sidenote{An essay on the work so far and its efficacy is currently under development, which can be found \href{https://github.com/probablytom/Albert-paper}{at http://bit.ly/2fZvggr}.} A full suite of design patterns have been developed, with example stories which follow the patterns. The intention is to provide a framework for parents to share stories with their children which have some sentimental value as a result of the improvisation and random aspect, as well as to provide a way for parents to connect through stories when they cannot necessarily afford books to read from. 

\nobibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
