\include{preamble}
\title{Anthropomorphic Algorithms for AI Safety}
\author[Tom Wallis]{\href{http://tom.coffee}{Tom Wallis}}
\date{}

\begin{document}

\maketitle

\section{Anthopomorphic algorithms}
\smallcaps{For a long time}, sociotechnical systems analysis within computing science has been developing formalisms of human-like traits, such as trust and comfort. These allow an intelligent agent to interact with other agents in its environment in measured, cautious ways; they might be used, for example, to decide whether it should accept information from another agent if its behaviour is becoming erratic (or to discard previous data which is no longer ``trustworthy''). \par

These anthopomorphic algorithms are undergoing continual improvement\cite{Kramdi}\cite{Urbano2014}, but two problems remain untouched:
\begin{enumerate}
    \item Various different anthropmorphic algorithms have been developed, but none have been combined into a system with several traits. {\newline}For example, an algorithm might be designed where an agent's ratings of trust and comfort in a given scenario influence each other --- not unlike a human's lack of trust in an agent making it less comfortable with certain situations.
    \item Lots of philosophical questions arise when developing anthopomorphic agents. There are questions in roboethics, such as the morality of creating an intelligent agent which might exhibit racial bias after its training. There are also questions in machine ethics, involving ethical decisions that an agent might make, and how they can be affected by trust and comfort.
\end{enumerate}

\bigskip
\smallcaps{However}, one exciting unexplored problem is that of AI safety: could anthopomorphic algorithms give humanity an edge in developing friendly AI\@? If we can limit its space of mind\cite{shanahan, sloman_spaceofminds}, perhaps we can limit potential damage from an artificial agent. Perhaps developing anthropomorphic agents allows us to reason better about how an artificial agent can be unfriendly, allowing us to better predict catastrophies related to the agent turning malicious. Perhaps such a method is provably inadequate for solving the problem of AI safety. In any case, further research is required to determine the method's efficacy.\par

\section{Responsibility and its implications}
% The affect of responsibility
\smallcaps{As a masters student} at Glasgow University, my current research is in developing a computational formalism of responsibility. This formalism would be the first algorithmic definition of an agent's responsibility, and fits the above problems perfectly. Responsibility has a wealth of philosophical literature already at its disposal, making it an ideal platform for beginning to address problems of machine ethics and roboethics. The formalism is similar to current trust and comfort models, making it easy to integrate with existing frameworks into an agent with several anthropomorphic traits. Most interestingly, an intelligent agent with a concept of responsibility is useful to analyse from the perspective of AI safety in a way that trust and comfort models are less suitable.\par

% Work that can be done now
As a result of the wealth of literature on responsibility for human agents, much work can be done to teach artificial agents to act in responsible ways; either in developing machine learning algorithms which tune the parameters of an agent's feeling of responsibility, or in imposing a strict sense of repsonsibility on that agent.\marginnote{One interesting question which springs to mind is that of space of possible minds. Earlier it was alluded to that anthropomorphic algorithms might limit the space of possible minds that an intelligent agent can occupy; computational responsibility, on the other hand, might broaden the space of possible responsibilities that an intelligent agent might inhabit. These agents are not bound by human instincts like self preservation and can be expected to ``value'' different things, making a theory of moral philosophy for a broader range of minds than biological ones possible and practical.}  That computational responsibility might improve agent corrigability\cite{corrigability} is an exciting prospect. Computational responsibility also provides an analogous framework to existing human responsibility to begin reasoning about how intelligent agents might relate to traditional moral responsibility.Plenty of work relating to existing philosophical literature becomes applicable to intelligent agents on the advent of computational responsibility.\par

\smallcaps{I propose} that the breadth and practicality of this work represents a substantial addition to the current literature on intelligent agents, 

\section{My suitability}

% my model of computational responsibility

% my experience with AI, sociotechnical systems and research

\nobibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
